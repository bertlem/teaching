{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815549be-f64d-4bd3-aeb3-3d4be21a31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ff801-07a4-41bf-81ac-f520cc7907ce",
   "metadata": {},
   "source": [
    "Goals:\n",
    "- Use encoding methods studied previously\n",
    "- Build a more complicated scikit-learn pipeline\n",
    "- Discover feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791aba53-b99c-41b7-8856-8b6701186132",
   "metadata": {},
   "source": [
    "# 1. Dataset quick outlook and preformating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25073a-6ccd-4be3-b1e7-0fc0125d06be",
   "metadata": {},
   "source": [
    "## 1.1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592c8d78-1ffd-4290-8aaf-5074c621f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset titanic\n",
    "path_data = '/home/lemasle/PROJECTS/data/Kaggle_Titanic-Machine_Learning_from_Disaster/'\n",
    "# Some columns include nans, we import them as strings\n",
    "input_train = pd.read_csv(path_data+'train.csv', na_values=['NaN'])\n",
    "input_test = pd.read_csv(path_data+'test.csv', na_values=['NaN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b47267-0b32-42ce-8340-b5bcef9cd320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6927f42-e37f-4cd3-b99a-60492b47c9be",
   "metadata": {},
   "source": [
    "PassengerId\n",
    "Survived: Survival \t (0 = No, 1 = Yes)\\\n",
    "Pclass:   Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\\\n",
    "Name:     Name\\\n",
    "Sex:      Sex\\\n",
    "Age:      Age in years\\\n",
    "Sibsp:    Number of siblings/spouses aboard the Titanic\\\n",
    "Parch:    Number of parents / children aboard the Titanic\\\n",
    "Ticket:   Ticket number\\\n",
    "Fare:     Passenger fare\\\n",
    "Cabin:    Cabin number\\\n",
    "Embarked: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20745a1-a843-4ca8-82da-b601d88f6487",
   "metadata": {},
   "source": [
    "## 1.2 Preformatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fea89f-28c0-46e5-bda6-a945f0e02cc3",
   "metadata": {},
   "source": [
    "### 1.2.1 Removing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e757b9-d1da-4e41-b767-8cfc50da0a3e",
   "metadata": {},
   "source": [
    "\"PassengerId\" is an internal label of the dataset - it can be discarded.\\\n",
    "\"Ticket\"  is simply a label, with no influence on the survival. The ticket label may refer to the location/price of the cabin, but this information is inluded in te \"Fare\" and \"Cabin\" columns: So we discard \"Ticket as well.\"\\\n",
    "\"Name\" should not have any influence on the survival, we decide to discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1277d9f7-d80d-49ba-a066-f75eeb33ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(df, list_of_columns):\n",
    "\n",
    "    # list_of_columns = [\"PassengerId\", \"Name\", \"Ticket\"]\n",
    "    df.drop(columns=list_of_columns, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab7e4a-3959-4dfb-b474-bc11d4a637f1",
   "metadata": {},
   "source": [
    "### 1.2.2 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900085a-48cd-48f5-a66e-5e224f9b3d8c",
   "metadata": {},
   "source": [
    "Note however that it contains some information, sometimes redundant (Mr, Mrs), sometimes not (Dr, Rev). We will use this information to create a new feature: 'Title'.\\\n",
    "The structure of strings in the first columns is as follows:\n",
    "\n",
    "[Lastname]  [comma]  [Title]  [dot]  [List of firstnames]\n",
    "\n",
    "So we can split each string accordingly and select the item between \"comma\" and \"dot\" as the title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4d4078-4101-47d3-a106-863a669eba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_feature(df):\n",
    "\n",
    "    # split info in df['Name'] and get title \n",
    "    title = [i.split(\",\")[1].split(\".\")[0].strip() for i in df[\"Name\"]]\n",
    "    # Convert the list in a series and include it in the original dataframe\n",
    "    df[\"Title\"] = pd.Series(title)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b2e1e-c10c-469b-90cc-6bdaac7a967b",
   "metadata": {},
   "source": [
    "### 1.2.3 Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8072da-ca64-43ef-9ccd-83d4592215aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "input_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dae43-ee63-47c8-86fb-4788ecf88858",
   "metadata": {},
   "source": [
    "#### 1.2.3.1 Missing data: some more feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95a32a-17ee-46bc-b2e4-7e04f381abf6",
   "metadata": {},
   "source": [
    "\"Cabin\" contains a lot (almost 700) of missing values.\\\n",
    "We could remove the entire column, but we would then loose some (potentially useful) information.\\\n",
    "Instead, we can transform this columns to a boolean indicating whether cabin information is available (1) or not (0).\\\n",
    "This is not much information, but better than nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabcab5f-0106-4d69-ac40-a4ba0be31873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_to_boolean(df):\n",
    "    \n",
    "    df['Cabin'] = np.where(df['Cabin'].isna(), 0.0, 1.0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5eba40-d2f7-490f-9f0e-c89b56f328d0",
   "metadata": {},
   "source": [
    "It is possible to transform a function so that it can be easily integrated into a pipeline. This is achieved via the FunctionTransformer command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8609af-dcd5-46a5-9e7e-ae7880c52e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "CabinTransformer = FunctionTransformer(cabin_to_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75d6b7c-aab1-4e08-b67c-5fb3428a2181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare  Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500    0.0        S  \n",
       "1        0          PC 17599  71.2833    1.0        C  \n",
       "2        0  STON/O2. 3101282   7.9250    0.0        S  \n",
       "3        0            113803  53.1000    1.0        S  \n",
       "4        0            373450   8.0500    0.0        S  \n",
       "..     ...               ...      ...    ...      ...  \n",
       "886      0            211536  13.0000    0.0        S  \n",
       "887      0            112053  30.0000    1.0        S  \n",
       "888      2        W./C. 6607  23.4500    0.0        S  \n",
       "889      0            111369  30.0000    1.0        C  \n",
       "890      0            370376   7.7500    0.0        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cabin = CabinTransformer.transform(input_train)\n",
    "test_cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41bb21-11be-4e2e-8d0c-6e6400a319cb",
   "metadata": {},
   "source": [
    "#### 1.2.3.2 Missing data: categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc675c5-1ef1-4507-b20e-0a3ef928e1c0",
   "metadata": {},
   "source": [
    "\"Embarked\" contains only two missing values.\\\n",
    "We could always consider removing the two corresponding rows, which would have little impact if they correspond to well-represented passenger profiles, but a higher impact if they happen to be rare occurences of a specific profile.\\\n",
    "Instead, we can impute them the most frequent embarkment spot, the hypotheses being that i) we still have a good chance that this arbitrary assignment is correct ii) the model will still perform better with two potentially wrong embarkment spots than with two full passenger profiles deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5472ce4d-a4cc-4115-abd9-59846500da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_most_frequent(df):\n",
    "\n",
    "    # Get the mode of a set of values, that is the value that appears most often\n",
    "    embarked_mode = df['Embarked'].mode()\n",
    "    # fill NaNs with the most common value\n",
    "    df['Embarked'].fillna(embarked_mode.values[0], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc0c99-ea55-40a9-93cc-80506707d8f6",
   "metadata": {},
   "source": [
    "Note: In the train sample, only \"embarked\" contains missing data. It could happen that in the test sample, also other categorical columns ('Sex', 'Pclass') contain missing values. We can use the same strategy (most frequent) to replace missing values for those columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562cebd-d302-4a20-9c9b-24689a4fa95e",
   "metadata": {},
   "source": [
    "Instead of using pandas functions, we will take advantage of scikit-learn SimpleImputer:\n",
    "1. that can be applied to several columns\n",
    "2. they can be integrated in a scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f88691f-7f7d-4ec2-a5fc-b5fa18bbf4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sex       891 non-null    object\n",
      " 1   Pclass    891 non-null    int64 \n",
      " 2   Embarked  889 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 21.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Get data for testing the method\n",
    "test_freq_imputer = input_train[['Sex', 'Pclass', 'Embarked']]\n",
    "test_freq_imputer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f27425c-bb14-4de4-97b0-8af8f2b266af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and define the imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "mostfrequent_imputer = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268f80f4-e1ae-4d9e-ae05-bc1fb43d2f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sex       891 non-null    object\n",
      " 1   Pclass    891 non-null    object\n",
      " 2   Embarked  891 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 21.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# fit the imputer on the input data\n",
    "mostfrequent_imputer.fit(test_freq_imputer)\n",
    "# impute all missing values in the data.\n",
    "X_frq = mostfrequent_imputer.transform(test_freq_imputer)\n",
    "# inspect the results, check that missing values have been replaced\n",
    "df_frq = pd.DataFrame(X_frq, columns=test_freq_imputer.columns, index=test_freq_imputer.index)\n",
    "df_frq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89704ee8-8cbf-4b7f-92b5-45fd1e74df29",
   "metadata": {},
   "source": [
    "#### 1.2.3.3 Missing data: numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa48d15-76c9-4d3b-a8f1-fefbd7b5ea5c",
   "metadata": {},
   "source": [
    "\"Age\" contains a good number (a bit more than 150) of missing values (NaNs).\\\n",
    "However 2/3rd of the rows contain potentially useful data (very young/very old people probably have a lower chance of survival), so it would be painful to remove the entire column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2f9ff-5cb5-49c2-a842-2d0cd4ea8257",
   "metadata": {},
   "source": [
    "One possibility is to fill missing values with the mean or median age (the median is less sensitive to large outliers than the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f2e775-d425-4017-9b4f-42446aab9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_median(df):\n",
    "\n",
    "    # get the median age\n",
    "    median_age = df['Age'].median()\n",
    "    # fill NaNs with the median value\n",
    "    df['Age'].fillna(median_age, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f81d9c-3001-4e1c-ab90-ed861431398c",
   "metadata": {},
   "source": [
    "Note: One should always be careful while introducing operations like mean() or median() in the early stages of a pipeline, as it could induce data leakage: rows in the train (respectively test) data would get clues about the ages in the test (respectively train) data. **In this specific example**, the train and test dataset are already split so we are not exposed to this risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd0924-b4e1-4030-8a57-2fd4091e4a58",
   "metadata": {},
   "source": [
    "Note: In the train sample, only \"age\" contains missing data. It could happen that in the test sample, also other numerical columns ('SibSp', 'Parch', 'Fare') contain missing values. We can use the same strategy (median value) to replace missing values for those columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11faca-ae84-42fa-b834-e202027fa64e",
   "metadata": {},
   "source": [
    "Instead of using pandas functions, we will take advantage of scikit-learn SimpleImputer:\n",
    "1. that can be applied to several columns\n",
    "2. they can be integrated in a scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7922b640-8448-4530-9ad1-ea4eb74115ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Age     714 non-null    float64\n",
      " 1   SibSp   891 non-null    int64  \n",
      " 2   Parch   891 non-null    int64  \n",
      " 3   Fare    891 non-null    float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 28.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Get data for testing the method\n",
    "test_median_imputer = input_train[['Age', 'SibSp', 'Parch', 'Fare']]\n",
    "test_median_imputer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42a8478f-9cef-490b-b2a7-e818aa0b1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and define the imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "median_imputer = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc70470e-9540-4e66-a882-8818f6c94362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Age     891 non-null    float64\n",
      " 1   SibSp   891 non-null    float64\n",
      " 2   Parch   891 non-null    float64\n",
      " 3   Fare    891 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 28.0 KB\n"
     ]
    }
   ],
   "source": [
    "# fit the imputer on the input data\n",
    "median_imputer.fit(test_median_imputer)\n",
    "# impute all missing values in the data.\n",
    "X_med = median_imputer.transform(test_median_imputer)\n",
    "# inspect the results, check that missing values have been replaced\n",
    "df_med = pd.DataFrame(X_med, columns=test_median_imputer.columns, index=test_median_imputer.index)\n",
    "df_med.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4ed54-cbd9-478a-bb7a-6326c25e4de5",
   "metadata": {},
   "source": [
    "## 1.3 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca909c3-50f0-4896-82f9-9ac93f16eb25",
   "metadata": {},
   "source": [
    "### 1.3.1 Ordinal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf7e8a-a756-49f3-953a-bd25cdfb59df",
   "metadata": {},
   "source": [
    "We will use ordinal encoding to encode the 'Sex' column, from 'Male'/'Female' to (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dfb685d-599b-4e73-9340-ecd053e20ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import and define the encoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "# fit and apply the encoder to the data\n",
    "sex_oe = ordinal_encoder.fit_transform(df_frq[['Sex']]) # Note the double [[ ]] as input data needs to be 2D\n",
    "sex_oe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560d975-3803-4685-8ef6-09f0ec18f5b2",
   "metadata": {},
   "source": [
    "We will use ordinal encoding to encode the 'Pclass' column, from (1,2,3) to (0,1,2).\\\n",
    "We want to preserve the order as 2nd class cabins lie in between 1st class cabins and 3rd class cabins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "765d343d-0bab-4885-a1c9-0579bd740e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import and define the encoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "# fit and apply the encoder to the data\n",
    "pclass_oe = ordinal_encoder.fit_transform(df_frq[['Pclass']]) # Note the double [[ ]] as input data needs to be 2D\n",
    "pclass_oe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504eed0f-36cd-43e2-b99f-0deec5ba897b",
   "metadata": {},
   "source": [
    "### 1.3.2 One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c0e5f-199a-4f84-ba73-a5d7d5a16abf",
   "metadata": {},
   "source": [
    "Although the inaugural trip of the Titanic started in Southampton, then went to Cherbourg, and finally to Queenstown, there is no specific order between the cities, expecially regarding the chances for survival.\\\n",
    "Thus One-Hot-Encoding is the best method for the 'Embarked' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1df5014-c643-45a7-a8cf-2730ca587708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import and define the encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "embarked_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "# fit and apply the encoder to the data\n",
    "embarked_ohe = embarked_encoder.fit_transform(df_frq[['Embarked']]) # Note the double [[ ]] as input data needs to be 2D\n",
    "embarked_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c243f-b24d-4302-8093-6a64ab96ac51",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: (handle_unknown=’ignore’ is specified to prevent errors with unseen categories in the testing set\\\n",
    "(categories present in the testing set but not present in the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238110c-e784-4493-a71a-c67c7512076f",
   "metadata": {},
   "source": [
    "### 1.3.3 More feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ddaa7-0f7c-40d4-88f3-268cac80d7d1",
   "metadata": {},
   "source": [
    "#### 1.3.3.1: Traveling alone or with families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7242ad88-d32d-490b-8fa4-0d11c1e56419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SibSp\n",
       "0    608\n",
       "1    209\n",
       "2     28\n",
       "4     18\n",
       "3     16\n",
       "8      7\n",
       "5      5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train['SibSp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef58cdb9-a28c-4c6d-97ca-556494d6cd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parch\n",
       "0    678\n",
       "1    118\n",
       "2     80\n",
       "5      5\n",
       "3      5\n",
       "4      4\n",
       "6      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train['Parch'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b41b4a-2914-4b28-9fff-5b76125e76be",
   "metadata": {},
   "source": [
    "Both \"Sibsp\" (number of siblings/spouses aboard) and \"Parch\" (number of parents/children aboard) contain 7 unique categories, but data are concentrated in 2+3 highly populated categories while some others contain only a handul objects.\\\n",
    "There is therefore a risk of overfitting. More importantly, encoding via one-hot-encoding would replace two columns with 14 new ones, and such a strong increase of dimensionality is probably not a good thing.\\\n",
    "For the sake of the exercise, we will also perform some feature engineering here, simply asking whether people travel alone or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "803f7717-df8f-40b5-9dab-dff4359e06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Is_Alone(df):\n",
    "\n",
    "    df['Is_Alone'] = np.where( (df['SibSp'] + df['Parch'])>0, 0.0, 1.0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a091e47-d454-437d-8b59-a3ccf52d2532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is_Alone\n",
       "1.0    537\n",
       "0.0    354\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Is_Alone(input_train)['Is_Alone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cc3ed6f-0517-43ec-84cf-2e8306d22344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is not useful to import FunctionTransformer again\n",
    "# I show it again for clarity \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "FamilyTransformer = FunctionTransformer(Is_Alone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78765d5f-cc95-4c1e-98c5-3609a879da4e",
   "metadata": {},
   "source": [
    "#### 1.3.3.2: Revisiting and encoding 'Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7e3e20d-e33c-4aec-b9f5-60900fc56df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "Mr              517\n",
       "Miss            182\n",
       "Mrs             125\n",
       "Master           40\n",
       "Dr                7\n",
       "Rev               6\n",
       "Mlle              2\n",
       "Major             2\n",
       "Col               2\n",
       "the Countess      1\n",
       "Capt              1\n",
       "Ms                1\n",
       "Sir               1\n",
       "Lady              1\n",
       "Mme               1\n",
       "Don               1\n",
       "Jonkheer          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_title = pd.DataFrame(get_title_feature(input_train), columns=['Title'])\n",
    "test_title['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a35f04-7690-4d23-9852-c826d16b5a08",
   "metadata": {},
   "source": [
    "A number of those are duplicates due to convention (Ms=Mrs) or language (Mme=Mrs, Don=Sir). We will simplify the 'Title' column by mapping a dictionnary before encoding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53b4ecf7-017c-488c-b240-f2ab0a4b15e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.  ,  7.  , 11.  ,  4.  ,   nan,  0.83, 12.  ,  1.  ,  9.  ,\n",
       "        3.  ,  0.92,  8.  ,  6.  ,  0.67,  0.42, 10.  ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train.loc[input_train.Title=='Master']['Age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb114071-509c-4bea-b183-cad7846d3511",
   "metadata": {},
   "source": [
    "If we inspect ages of passengers with title 'Master', we discover that all of them are kids.\\\n",
    "In passing, the presence of NaNs indicates that we could do a more clever job imputing ages than simply taking the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07651c2b-e432-4a73-9e88-68a4bf246210",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_dict = {'Mr': 'Mr',\n",
    "              'Miss' : 'Miss',\n",
    "              'Mrs' : 'Mrs',\n",
    "              'Master' : 'Master',\n",
    "              'Dr' : 'Other',\n",
    "              'Rev' : 'Other',\n",
    "              'Mlle' : 'Miss',\n",
    "              'Major' : 'Other',\n",
    "              'Col' : 'Other',\n",
    "              'the Countess' : 'Noble',\n",
    "              'Capt' : 'Other',\n",
    "              'Ms' : 'Mrs',\n",
    "              'Sir' : 'Noble',\n",
    "              'Lady' : 'Noble',\n",
    "              'Mme' : 'Mrs',\n",
    "              'Don' : 'Noble',\n",
    "              'Jonkheer' : 'Noble',\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "facd5f8c-4250-4a84-ac89-dd58efa12887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "Mr        517\n",
       "Miss      184\n",
       "Mrs       127\n",
       "Master     40\n",
       "Other      18\n",
       "Noble       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_title['Title'] = test_title['Title'].map(Title_dict)\n",
    "test_title['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11b0d38d-fd53-4295-99f5-8527244ad9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import and define the encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "titles_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "# fit and apply the encoder to the data\n",
    "titles_ohe = titles_encoder.fit_transform(test_title[['Title']]) # Note the double [[ ]] as input data needs to be 2D\n",
    "print(np.shape(titles_ohe))\n",
    "titles_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d2adf4-2872-4f50-9a22-c5f0b5aeb65b",
   "metadata": {},
   "source": [
    "## 1.4 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da723-d271-4ba7-95f8-7ed3a7d640be",
   "metadata": {},
   "source": [
    "- Feature engineering\n",
    " 1. get_title_feature\n",
    " 2. Is_Alone\n",
    "- Missing data\n",
    " 1. fillna_with_most_frequent\n",
    " 2. fillna_with_median\n",
    "- Encoding\n",
    " 1. sex_oe\n",
    " 2. Pclass_oe\n",
    " 3. cabin_to_boolean\n",
    " 4. embarked_ohe\n",
    " 5. titles_ohe\n",
    "\n",
    "- remove columns (those not considered at all or modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c895b09-d25e-437c-87d8-061ead8f552c",
   "metadata": {},
   "source": [
    "# 2. Pipeline building: columns transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e1e5e-7336-4d07-b3b7-c231791dac93",
   "metadata": {},
   "source": [
    "In order to group all the individual steps listed above, we will have to:\n",
    " 1. Include each of them inside a Pipeline instance\n",
    " 2. Group these pipeline instances using ColumnTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fcd3894-28f4-4b76-89a2-7c8a9bdf6e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3689844948.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[32], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    ('HasCabin',CabinTransformer())\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# cabin_pipeline applies to 'Cabin'\n",
    "# the column is replaced\n",
    "cabin_pipeline = Pipeline(steps=[\n",
    "    ('HasCabin',CabinTransformer())\n",
    "    ])\n",
    "cabin_cols = ['Cabin']\n",
    "    \n",
    "# family_pipeline applies to 'Sibsp', 'Parch'\n",
    "# these columns are not used anymore    \n",
    "family_pipeline= Pipeline(steps=[\n",
    "    ('HasFamily',FamilyTransformer())\n",
    "    ])\n",
    "family_cols = ['Sibsp', 'Parch']\n",
    "\n",
    "# num_pipeline applies to 'Age', 'Fare'\n",
    "# Missing values are median-imputed\n",
    "# We will also apply a standard scaler\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale',MinMaxScaler())\n",
    "    ])\n",
    "num_cols = ['Age', 'Fare']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2288843-5412-44ef-9b85-66284ac7d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD columns to remove: PassengerId 'Sibsp', 'Parch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8217c1-5aa4-4a47-94e8-3bea2e288049",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale',MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7bb0f-914d-477d-aa3e-f58b4ba86601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c6bb5539-ae5e-4a7f-a394-07869b5c7786",
   "metadata": {},
   "source": [
    "    PassengerId\n",
    "Survived: Survival \t (0 = No, 1 = Yes)\\\n",
    "Pclass:   Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\\\n",
    "Name:     Name\\\n",
    "Sex:      Sex\\\n",
    "    Age:      Age in years\\\n",
    "    Sibsp:    Number of siblings/spouses aboard the Titanic\\\n",
    "    Parch:    Number of parents / children aboard the Titanic\\\n",
    "Ticket:   Ticket number\\\n",
    "    Fare:     Passenger fare\\\n",
    "    Cabin:    Cabin number\\\n",
    "Embarked: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c5f96-9029-42cf-b062-eb0c08545564",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3 Using ColumnTransfomer to group the two branches of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e66cf-eb83-47d6-8ee5-ece965b5c191",
   "metadata": {},
   "source": [
    "The syntax of a ColumnTransformer is as follows:\n",
    "\n",
    "ColumnTransformer(transformers=[(‘step name’, transform function,cols), …])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45d9bd-6edb-46e1-8460-28fed7fbbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col_trans = ColumnTransformer(transformers=[\n",
    "    ('num_pipeline',num_pipeline,num_cols),\n",
    "    ('cat_pipeline',cat_pipeline,cat_cols)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0572e5-d654-42b0-a453-bc8237c70291",
   "metadata": {},
   "source": [
    "Note 1: remainder=’drop’ is specified to ignore the other dataframe columns.\\\n",
    "Note 2: n_job = -1 to use all processors in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ac624d3-6e71-403c-ac74-fb841670cf1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5707ba-9fd8-425f-8e85-9a613b02798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_title_feature(input_train) # TBD do feature engineering here tooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d086d-f5c6-4fc5-99ea-ab343685293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preformat(df_init):\n",
    "    df = df_init.copy()\n",
    "    df.loc[:,'relevent_experience'] = df['relevent_experience'].map(relevant_experience_map)\n",
    "    df.loc[:,'last_new_job'] = df['last_new_job'].map(last_new_job_map)\n",
    "    df.loc[:,'experience'] = df['experience'].map(experience_map)\n",
    "\n",
    "    return dfdf.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d709ff2-aaa9-4d43-9a21-7a8b975bebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "preformater = FunctionTransformer(preformat)\n",
    "test = preformater.transform(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
